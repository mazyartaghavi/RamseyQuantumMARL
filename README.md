# Ramsey-Guided Quantum Optimization for Explorationâ€“Exploitation in Multi-Agent Reinforcement Learning (QIO-MARL)

QIO-MARL is a research codebase that implements **quantum-inspired operator updates**
and **entropy-regularized control** for multi-agent reinforcement learning under partial
observability. It includes:
- A simple cooperative UAV forest-monitoring environment (grid, partial obs, limited comms)
- Quantum-inspired operator `ğ’¬` (amplitude-amplificationâ€“style reweighting of logits)
- Entropy annealing (classical + quantum-inspired decay law)
- A minimal Actorâ€“Critic (shared parameters) training loop for N agents
- Reproducible config, logging, and unit test for `ğ’¬`.

> This repo is intentionally small and pedagogicalâ€”ideal as a starting point to reproduce
> trends and extend for larger benchmarks (SMAC/MPE).

## Quickstart

```bash
# 1) Clone or copy this repository
python -m venv .venv && source .venv/bin/activate  # (on Windows: .venv\Scripts\activate)
pip install -r requirements.txt
Artifacts (plots, CSV logs, checkpoints) are saved under runs/<timestamp>/.

Core Ideas

Random operators + contraction in expectation.
We model each update as a random operator; the expected operator is contractive,
yielding almost-sure convergence under standard boundedness and measurability assumptions.

Quantum-inspired operator (ğ’¬).
We reweight action logits with an amplitude-style map on probabilities:

Accentuates high-probability actions while preserving a min entropy floor

Plays well with entropy regularization to avoid premature collapse

Entropy control.
We provide both classical exponential decay and a â€œquantum-inspiredâ€ aggregated decay:

ğ›¼
ğ‘¡
=
ğ›¼
0
exp
â¡
â€‰â£
(
âˆ’
ğœ†
âˆ‘
ğ‘˜
=
1
ğ‘¡
1
âˆ’
ğ»
ğ‘˜
2
)
Î±
t
	â€‹

=Î±
0
	â€‹

exp(âˆ’Î»
k=1
âˆ‘
t
	â€‹

1âˆ’H
k
2
	â€‹

	â€‹

)
Repository Contents

qio_marl/agents/policy.py â€” Shared-parameter Actorâ€“Critic (MLP)

qio_marl/agents/qio_operator.py â€” Quantum-inspired operator apply_q_operator

qio_marl/algos/qio_marl.py â€” Training update (entropy-regularized A2C + ğ’¬)

qio_marl/envs/forest_uav.py â€” Toy UAV grid env (partial obs, coverage reward)

qio_marl/utils/* â€” Logger, schedules, replay buffer

scripts/train_uav.py â€” CLI experiment runner

configs/uav_default.yaml â€” Default configuration

tests/test_q_operator.py â€” Unit test for ğ’¬

Configuration

See configs/uav_default.yaml for:

env: grid size, #agents, obs radius, episode length

algo: learning rates, gamma, entropy schedule, ğ’¬-operator hyperparams

train: total steps, log interval, seed
# 2) Train on the UAV forest environment (10 agents, small grid)
python scripts/train_uav.py --config configs/uav_default.yaml

# 3) (Optional) Run unit test for the quantum-inspired operator
pytest -q

